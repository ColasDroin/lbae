{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1\n",
    "Raw data export into np.memaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load important modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard modules\n",
    "import numpy as np\n",
    "import os\n",
    "import lzma\n",
    "import pickle\n",
    "\n",
    "# Move to root directory for easier module handling\n",
    "os.chdir(\"../..\")\n",
    "print(os.listdir(\".\"))\n",
    "from modules.tools import maldi_conversion \n",
    "from modules.tools import lookup_tables\n",
    "from modules.tools.misc import delete_all_files_in_folder\n",
    "\n",
    "# multithreading/multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "# set thread limit\n",
    "threadpool_limits(16)\n",
    "\n",
    "# Define if the app uses only MAIA-transformed lipids\n",
    "maldi_conversion.SAMPLE_APP = False\n",
    "if maldi_conversion.SAMPLE_APP:\n",
    "    lookup_tables.DIVIDER_LOOKUP = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of raw data filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_brain_1 = \"/data/lipidatlas/data/data_raw/BRAIN1/\"\n",
    "path_brain_2 = \"/data/lipidatlas/data/data_raw/BRAIN2/\"\n",
    "path_brain_1_temp = \"/data/lipidatlas/data/app/data/temp/brain_1\"\n",
    "path_brain_2_temp = \"/data/lipidatlas/data/app/data/temp/brain_2\"\n",
    "split_value_1 = \"MouseBrainCMC_S\"\n",
    "split_value_2 = \"MouseBrain2_S\"\n",
    "ll_t_names = []\n",
    "for path_brain, path_brain_temp, split_value in zip(\n",
    "    [path_brain_1, path_brain_2],\n",
    "    [path_brain_1_temp, path_brain_2_temp],\n",
    "    [split_value_1, split_value_2],\n",
    "):\n",
    "    # Load filenames\n",
    "    l_t_names = sorted(\n",
    "        [\n",
    "            [\n",
    "                int(name.split(split_value)[1].split(\"_\")[0].split(\"A\")[0].split(\"(\")[0]),\n",
    "                path_brain + name + \"/\" + name,\n",
    "            ]\n",
    "            for name in os.listdir(path_brain)\n",
    "            if \"MouseBrain\" in name\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Correct for duplicates\n",
    "    for t_names_1, t_names_2 in zip(l_t_names[:-1], l_t_names[1:]):\n",
    "        if t_names_2[0] == t_names_1[0]:\n",
    "            t_names_2.append(\"bis\")\n",
    "            print(\"WARNING: duplicate for slice \" + str(t_names_1[0]))\n",
    "\n",
    "    # Remove slices that have already been processed\n",
    "    os.makedirs(path_brain_temp, exist_ok=True)\n",
    "    remove_already_loaded = False\n",
    "    if remove_already_loaded:\n",
    "        existing_names = [\n",
    "            int(name.split(\"_\")[1][:-7]) for name in os.listdir(path_brain_temp) if \"raw\" in name\n",
    "        ]\n",
    "        l_t_names = [x for x in l_t_names if x[0] not in existing_names]\n",
    "\n",
    "    # Print the final list of names\n",
    "    for t_names in l_t_names:\n",
    "        print(t_names[0], t_names[1].split(\"/\")[-1])\n",
    "\n",
    "    ll_t_names.append(l_t_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_1 = True\n",
    "if brain_1:\n",
    "    l_t_names = ll_t_names[0]\n",
    "else:\n",
    "    l_t_names = ll_t_names[1]\n",
    "\n",
    "# Print the final list of names\n",
    "for t_names in l_t_names:\n",
    "    print(t_names[0], t_names[1].split(\"/\")[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Extract raw data into numpy arrays with multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    multiprocessing = True\n",
    "    if multiprocessing:\n",
    "        with Pool(processes=3) as pool:\n",
    "            [x for x in pool.imap_unordered(maldi_conversion.extract_raw_data, l_t_names)]\n",
    "    else:\n",
    "        # Normal (single-processed) map\n",
    "        [x for x in map(maldi_conversion.extract_raw_data, l_t_names)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove slices already processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    path_brain_temp = (\n",
    "        \"/data/lipidatlas/data/app/data/temp/brain_1\"\n",
    "        if brain_1\n",
    "        else \"/data/lipidatlas/data/app/data/temp/brain_2\"\n",
    "    )\n",
    "    existing_names = [\n",
    "        int(name.split(\"_\")[1][:-4]) for name in os.listdir(path_brain_temp) if \"raw\" not in name\n",
    "    ]\n",
    "    l_t_names = [x for x in l_t_names if x[0] not in existing_names]\n",
    "    # Print the final list of names\n",
    "    for t_names in l_t_names:\n",
    "        print(t_names[0], t_names[1].split(\"/\")[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process raw data into numpy arrays with multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing = True\n",
    "if multiprocessing:\n",
    "    with Pool(processes=12) as pool:\n",
    "        [x for x in pool.imap_unordered(maldi_conversion.process_raw_data, l_t_names)]\n",
    "else:\n",
    "    # Normal (single-processed) map\n",
    "    [x for x in map(maldi_conversion.process_raw_data, l_t_names)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing = True\n",
    "if multiprocessing:\n",
    "    # Multiprocessing\n",
    "    with Pool(processes=12) as pool:\n",
    "        [x for x in pool.map(lookup_tables.process_lookup_tables, l_t_names)]\n",
    "else:\n",
    "    # Normal (single-processed) map\n",
    "    [x for x in map(lookup_tables.process_lookup_tables, l_t_names)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record everything and clean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record everything in memap files and a pickled dictonnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if maldi_conversion.SAMPLE_APP:\n",
    "    output_folder = \"data_sample/whole_dataset/\"\n",
    "else :\n",
    "    output_folder = \"data/whole_dataset/\"  \n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "dic_slices = {}\n",
    "# Loop over input folders\n",
    "for brain_1, input_folder in zip(\n",
    "    [True, False],\n",
    "    [\n",
    "        \"/data/lipidatlas/data/app/data/temp/brain_1/\",\n",
    "        \"/data/lipidatlas/data/app/data/temp/brain_2/\",\n",
    "    ],\n",
    "):\n",
    "\n",
    "    # Loop over slice files\n",
    "    for slice_name in os.listdir(input_folder):\n",
    "        if \"raw\" in slice_name or \"checkpoints\" in slice_name:\n",
    "            continue\n",
    "\n",
    "        # Extract slice index\n",
    "        slice_index = int(slice_name.split(\"_\")[1][:-4])\n",
    "\n",
    "        # Load slice arrays\n",
    "        npzfile = np.load(input_folder + slice_name)\n",
    "        array_pixel_indexes_high_res = npzfile[\"array_pixel_indexes_high_res\"]\n",
    "        array_spectra_high_res = npzfile[\"array_spectra_high_res\"]\n",
    "        array_averaged_mz_intensity_low_res = npzfile[\"array_averaged_mz_intensity_low_res\"]\n",
    "        array_averaged_mz_intensity_high_res = npzfile[\"array_averaged_mz_intensity_high_res\"]\n",
    "        array_averaged_mz_intensity_high_res_after_standardization = npzfile[\n",
    "            \"array_averaged_mz_intensity_high_res_after_standardization\"\n",
    "        ]\n",
    "        image_shape = npzfile[\"image_shape\"]\n",
    "        divider_lookup = npzfile[\"divider_lookup\"]\n",
    "        lookup_table_spectra_high_res = npzfile[\"lookup_table_spectra_high_res\"]\n",
    "        cumulated_image_lookup_table_high_res = npzfile[\"cumulated_image_lookup_table_high_res\"]\n",
    "        lookup_table_averaged_spectrum_high_res = npzfile[\"lookup_table_averaged_spectrum_high_res\"]\n",
    "        array_peaks_corrected = npzfile[\"array_peaks_corrected\"]\n",
    "        array_corrective_factors = npzfile[\"array_corrective_factors\"]\n",
    "\n",
    "        # print size used by each array in mb\n",
    "        print(\"array_pixel_indexes_high_res, dic\",round(array_pixel_indexes_high_res.nbytes / 1024 / 1024, 2))\n",
    "        print(\"array_spectra_high_res, mmap\",round(array_spectra_high_res.nbytes / 1024 / 1024, 2))\n",
    "        print(\"array_averaged_mz_intensity_low_res, dic\",round(array_averaged_mz_intensity_low_res.nbytes / 1024 / 1024, 2))\n",
    "        print(\"array_averaged_mz_intensity_high_res, mmap\",round(array_averaged_mz_intensity_high_res.nbytes / 1024 / 1024, 2))\n",
    "        print(\"array_averaged_mz_intensity_high_res_after_standardization, mmap\",round(array_averaged_mz_intensity_high_res_after_standardization.nbytes / 1024 / 1024, 2))\n",
    "        print(\"lookup_table_spectra_high_res, mmap\",round(lookup_table_spectra_high_res.nbytes / 1024 / 1024, 2))\n",
    "        print(\"cumulated_image_lookup_table_high_res, mmap\",round(cumulated_image_lookup_table_high_res.nbytes / 1024 / 1024, 2))\n",
    "        print(\"lookup_table_averaged_spectrum_high_res, dic\",round(lookup_table_averaged_spectrum_high_res.nbytes / 1024 / 1024, 2))\n",
    "        print(\"array_peaks_corrected, dic\",round(array_peaks_corrected.nbytes / 1024 / 1024, 2))\n",
    "        print(\"array_corrective_factors, dic\",round(array_corrective_factors.nbytes / 1024 / 1024, 2))\n",
    "\n",
    "        # Update slice index for brain 2\n",
    "        if not brain_1:\n",
    "            slice_index += 22\n",
    "\n",
    "        print(slice_name)\n",
    "\n",
    "        if not maldi_conversion.SAMPLE_APP:\n",
    "            # Register the lightweights files in a pickled dictionnary\n",
    "            dic_slices[slice_index] = {\n",
    "                \"image_shape\": image_shape,\n",
    "                \"divider_lookup\": divider_lookup,\n",
    "                \"array_avg_spectrum_downsampled\": array_averaged_mz_intensity_low_res,\n",
    "                \"array_lookup_pixels\": array_pixel_indexes_high_res,\n",
    "                \"array_lookup_mz_avg\": lookup_table_averaged_spectrum_high_res,\n",
    "                \"array_peaks_transformed_lipids\": array_peaks_corrected,\n",
    "                \"is_brain_1\": brain_1,\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                # Build a memap for each of the heavier files to save RAM, save the corresponding shape in the\n",
    "                # pickled dictionnary\n",
    "                fp = np.memmap(\n",
    "                    output_folder + \"array_spectra_\" + str(slice_index) + \".mmap\",\n",
    "                    dtype=\"float32\",\n",
    "                    mode=\"w+\",\n",
    "                    shape=array_spectra_high_res.shape,\n",
    "                )\n",
    "                fp[:] = array_spectra_high_res[:]\n",
    "                fp.flush()\n",
    "                dic_slices[slice_index][\"array_spectra_shape\"] = array_spectra_high_res.shape\n",
    "\n",
    "                fp = np.memmap(\n",
    "                    output_folder + \"array_avg_spectrum_\" + str(slice_index) + \".mmap\",\n",
    "                    dtype=\"float32\",\n",
    "                    mode=\"w+\",\n",
    "                    shape=array_averaged_mz_intensity_high_res.shape,\n",
    "                )\n",
    "                fp[:] = array_averaged_mz_intensity_high_res[:]\n",
    "                fp.flush()\n",
    "                dic_slices[slice_index][\n",
    "                    \"array_avg_spectrum_shape\"\n",
    "                ] = array_averaged_mz_intensity_high_res.shape\n",
    "\n",
    "                fp = np.memmap(\n",
    "                    output_folder\n",
    "                    + \"array_avg_spectrum_after_standardization_\"\n",
    "                    + str(slice_index)\n",
    "                    + \".mmap\",\n",
    "                    dtype=\"float32\",\n",
    "                    mode=\"w+\",\n",
    "                    shape=array_averaged_mz_intensity_high_res_after_standardization.shape,\n",
    "                )\n",
    "                fp[:] = array_averaged_mz_intensity_high_res_after_standardization[:]\n",
    "                fp.flush()\n",
    "                dic_slices[slice_index][\n",
    "                    \"array_avg_spectrum_after_standardization_shape\"\n",
    "                ] = array_averaged_mz_intensity_high_res_after_standardization.shape\n",
    "\n",
    "                fp = np.memmap(\n",
    "                    output_folder + \"array_lookup_mz_\" + str(slice_index) + \".mmap\",\n",
    "                    dtype=\"int32\",\n",
    "                    mode=\"w+\",\n",
    "                    shape=lookup_table_spectra_high_res.shape,\n",
    "                )\n",
    "                fp[:] = lookup_table_spectra_high_res[:]\n",
    "                fp.flush()\n",
    "                dic_slices[slice_index][\"array_lookup_mz_shape\"] = lookup_table_spectra_high_res.shape\n",
    "\n",
    "                fp = np.memmap(\n",
    "                    output_folder + \"array_cumulated_lookup_mz_image_\" + str(slice_index) + \".mmap\",\n",
    "                    dtype=\"float32\",\n",
    "                    mode=\"w+\",\n",
    "                    shape=cumulated_image_lookup_table_high_res.shape,\n",
    "                )\n",
    "                fp[:] = cumulated_image_lookup_table_high_res[:]\n",
    "                fp.flush()\n",
    "                dic_slices[slice_index][\n",
    "                    \"array_cumulated_lookup_mz_image_shape\"\n",
    "                ] = cumulated_image_lookup_table_high_res.shape\n",
    "\n",
    "                fp = np.memmap(\n",
    "                    output_folder + \"array_corrective_factors_\" + str(slice_index) + \".mmap\",\n",
    "                    dtype=\"float32\",\n",
    "                    mode=\"w+\",\n",
    "                    shape=array_corrective_factors.shape,\n",
    "                )\n",
    "                fp[:] = array_corrective_factors[:]\n",
    "                fp.flush()\n",
    "                dic_slices[slice_index][\n",
    "                    \"array_corrective_factors_shape\"\n",
    "                ] = array_corrective_factors.shape\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        else:\n",
    "            # Register all files in a pickled dictionnary\n",
    "            dic_slices[slice_index] = {\n",
    "                \"image_shape\": image_shape,\n",
    "                \"divider_lookup\": divider_lookup,\n",
    "                \"array_avg_spectrum_downsampled\": array_averaged_mz_intensity_low_res,\n",
    "                \"array_lookup_pixels\": array_pixel_indexes_high_res,\n",
    "                \"array_lookup_mz_avg\": lookup_table_averaged_spectrum_high_res,\n",
    "                \"array_peaks_transformed_lipids\": array_peaks_corrected,\n",
    "                \"array_spectra\": array_spectra_high_res,\n",
    "                \"array_avg_spectrum\": array_averaged_mz_intensity_high_res,\n",
    "                \"array_avg_spectrum_after_standardization\": array_averaged_mz_intensity_high_res_after_standardization,\n",
    "                \"array_lookup_mz\": lookup_table_spectra_high_res,\n",
    "                \"array_cumulated_lookup_mz_image\": cumulated_image_lookup_table_high_res,\n",
    "                \"array_corrective_factors\": array_corrective_factors,\n",
    "                \"is_brain_1\": brain_1,\n",
    "            }\n",
    "\n",
    "\n",
    "if not maldi_conversion.SAMPLE_APP:\n",
    "    # Pickle the dict of lightweight data\n",
    "    with open(output_folder + \"light_arrays.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(dic_slices, handle)\n",
    "else:\n",
    "    with lzma.open(output_folder + \"light_arrays.pickle\", \"wb\") as handle:\n",
    "        pickle.dump(dic_slices, handle)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean temporary folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = False\n",
    "if clean:\n",
    "    delete_all_files_in_folder(input_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c1aa729cc35b9a783763c24c4069d7da678acf641f89d4e1df25bf02079ad65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
