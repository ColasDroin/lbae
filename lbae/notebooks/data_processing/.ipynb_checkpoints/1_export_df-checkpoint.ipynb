{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1\n",
    "Raw data export into np.memaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load important modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'pages', 'app.py', 'assets', 'config.py', 'index.py', 'main.py', 'TODO.py', 'notebooks', 'modules', '__pycache__']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<threadpoolctl.threadpool_limits at 0x7f11a91b3760>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard modules\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Move to root directory for easier module handling\n",
    "os.chdir(\"../..\")\n",
    "print(os.listdir(\".\"))\n",
    "from notebooks.data_processing.modules.maldi_conversion import process_raw_data, extract_raw_data\n",
    "from notebooks.data_processing.modules.lookup_tables import process_lookup_tables\n",
    "from modules.tools.misc import delete_all_files_in_folder\n",
    "\n",
    "# multithreading/multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from threadpoolctl import threadpool_limits\n",
    "\n",
    "# set thread limit\n",
    "threadpool_limits(16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of raw data filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filenames\n",
    "l_t_names = sorted(\n",
    "    [\n",
    "        [\n",
    "            int(name.split(\"MouseBrainCMC_S\")[1].split(\"_\")[0].split(\"A\")[0].split(\"(\")[0]),\n",
    "            \"/data/lipidatlas/data/data_raw/\" + name + \"/\" + name,\n",
    "        ]\n",
    "        for name in os.listdir(\"/data/lipidatlas/data/data_raw/\")\n",
    "        if \"MouseBrain\" in name\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Correct for duplicates\n",
    "for t_names_1, t_names_2 in zip(l_t_names[:-1], l_t_names[1:]):\n",
    "    if t_names_2[0] == t_names_1[0]:\n",
    "        t_names_2.append(\"bis\")\n",
    "        print(\"WARNING: duplicate for slice \" + str(t_names_1[0]))\n",
    "\n",
    "# Remove slices that have already been processed\n",
    "path = \"notebooks/data_processing/data/temp/\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "remove_already_loaded = True\n",
    "if remove_already_loaded:\n",
    "    existing_names = [int(name.split(\"_\")[1][:-7]) for name in os.listdir(path) if \"raw\" in name]\n",
    "    l_t_names = [x for x in l_t_names if x[0] not in existing_names]\n",
    "\n",
    "# Print the final list of names\n",
    "for t_names in l_t_names:\n",
    "    print(t_names[0], t_names[1].split(\"/\")[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract raw data into numpy arrays with multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The mzML file containts more than the expected84845 spectra. The tailing 5 spectra will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files : /data/lipidatlas/data/data_raw/20210501_MouseBrainCMC_S31_3_2Dpixelmode_355x239_Att30_25um/20210501_MouseBrainCMC_S31_3_2Dpixelmode_355x239_Att30_25um\n",
      "[Warning] Not index found and build_index_from_scratch is False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Sprectra at resolution 1e-05: 100%|██████████| 84845/84845 [00:56<00:00, 1510.29it/s]\n",
      "Loading the m/z values at resolution 1e-05: 100%|██████████| 84845/84845 [01:17<00:00, 1092.75it/s]\n",
      "WARNING:root:The sparsified dense matrix S has never been computed. Please wait while it's being generated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and sorting dataframes\n"
     ]
    }
   ],
   "source": [
    "multiprocessing = False\n",
    "if multiprocessing:\n",
    "    with Pool(processes=14) as pool:\n",
    "        [x for x in pool.imap_unordered(extract_raw_data, l_t_names)]\n",
    "else:\n",
    "    # Normal (single-processed) map\n",
    "    [x for x in map(extract_raw_data, l_t_names)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process raw data into numpy arrays with multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing = False\n",
    "if multiprocessing:\n",
    "    with Pool(processes=12) as pool:\n",
    "        [x for x in pool.imap_unordered(process_raw_data, l_t_names)]\n",
    "else:\n",
    "    # Normal (single-processed) map\n",
    "    [x for x in map(process_raw_data, l_t_names[:1])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiprocessing = True\n",
    "if multiprocessing:\n",
    "    # Multiprocessing\n",
    "    with Pool(processes=12) as pool:\n",
    "        [x for x in pool.map(process_lookup_tables, l_t_names)]\n",
    "else:\n",
    "    # Normal (single-processed) map\n",
    "    [x for x in map(process_lookup_tables, l_t_names)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record everything and clean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record everything in memap files and a pickled dictonnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"data/whole_dataset/\"\n",
    "input_folder = \"notebooks/data_processing/data/temp/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "dic_slices = {}\n",
    "# Loop over slice files\n",
    "for slice_name in os.listdir(input_folder):\n",
    "\n",
    "    # Extract slice index\n",
    "    slice_index = int(slice_name.split(\"_\")[1][:-4])\n",
    "\n",
    "    # Load slice arrays\n",
    "    npzfile = np.load(input_folder + slice_name)\n",
    "    array_pixel_indexes_high_res = npzfile[\"array_pixel_indexes_high_res\"]\n",
    "    array_spectra_high_res = npzfile[\"array_spectra_high_res\"]\n",
    "    array_averaged_mz_intensity_low_res = npzfile[\"array_averaged_mz_intensity_low_res\"]\n",
    "    array_averaged_mz_intensity_high_res = npzfile[\"array_averaged_mz_intensity_high_res\"]\n",
    "    image_shape = npzfile[\"image_shape\"]\n",
    "    divider_lookup = npzfile[\"divider_lookup\"]\n",
    "    lookup_table_spectra_high_res = npzfile[\"lookup_table_spectra_high_res\"]\n",
    "    cumulated_image_lookup_table_high_res = npzfile[\"cumulated_image_lookup_table_high_res\"]\n",
    "    lookup_table_averaged_spectrum_high_res = npzfile[\"lookup_table_averaged_spectrum_high_res\"]\n",
    "    array_peaks_corrected = npzfile[\"array_peaks_corrected\"]\n",
    "    array_corrective_factors = npzfile[\"array_corrective_factors\"]\n",
    "\n",
    "    # Print array size\n",
    "    # print size used by each array in mb\n",
    "    print(round(array_pixel_indexes_high_res.nbytes / 1024 / 1024, 2))\n",
    "    print(round(array_spectra_high_res.nbytes / 1024 / 1024, 2))\n",
    "    print(round(array_averaged_mz_intensity_low_res.nbytes / 1024 / 1024, 2))\n",
    "    print(round(array_averaged_mz_intensity_high_res.nbytes / 1024 / 1024, 2))\n",
    "    print(round(lookup_table_spectra_high_res.nbytes / 1024 / 1024, 2))\n",
    "    print(round(cumulated_image_lookup_table_high_res.nbytes / 1024 / 1024, 2))\n",
    "    print(round(lookup_table_averaged_spectrum_high_res.nbytes / 1024 / 1024, 2))\n",
    "\n",
    "    # Register the lightweights files in a pickled dictionnary\n",
    "    dic_slices[slice_index] = {\n",
    "        \"image_shape\": image_shape,\n",
    "        \"divider_lookup\": divider_lookup,\n",
    "        \"array_avg_spectrum_downsampled\": array_averaged_mz_intensity_low_res,\n",
    "        \"array_lookup_pixels\": array_pixel_indexes_high_res,\n",
    "        \"array_lookup_mz_avg\": lookup_table_averaged_spectrum_high_res,\n",
    "        \"array_peaks_transformed_lipids\": array_peaks_corrected,\n",
    "        \"array_corrective_factors\": array_corrective_factors,\n",
    "    }\n",
    "\n",
    "    # Build a memap for each of the heavier files to save RAM, save the corresponding shape in the\n",
    "    # pickled dictionnary\n",
    "    fp = np.memmap(\n",
    "        output_folder + \"array_spectra_\" + str(slice_index) + \".mmap\",\n",
    "        dtype=\"float32\",\n",
    "        mode=\"w+\",\n",
    "        shape=array_spectra_high_res.shape,\n",
    "    )\n",
    "    fp[:] = array_spectra_high_res[:]\n",
    "    fp.flush()\n",
    "    dic_slices[slice_index][\"array_spectra_shape\"] = array_spectra_high_res.shape\n",
    "\n",
    "    fp = np.memmap(\n",
    "        output_folder + \"array_avg_spectrum_\" + str(slice_index) + \".mmap\",\n",
    "        dtype=\"float32\",\n",
    "        mode=\"w+\",\n",
    "        shape=array_averaged_mz_intensity_high_res.shape,\n",
    "    )\n",
    "    fp[:] = array_averaged_mz_intensity_high_res[:]\n",
    "    fp.flush()\n",
    "    dic_slices[slice_index][\"array_avg_spectrum_shape\"] = array_averaged_mz_intensity_high_res.shape\n",
    "\n",
    "    fp = np.memmap(\n",
    "        output_folder + \"array_lookup_mz_\" + str(slice_index) + \".mmap\",\n",
    "        dtype=\"int32\",\n",
    "        mode=\"w+\",\n",
    "        shape=lookup_table_spectra_high_res.shape,\n",
    "    )\n",
    "    fp[:] = lookup_table_spectra_high_res[:]\n",
    "    fp.flush()\n",
    "    dic_slices[slice_index][\"array_lookup_mz_shape\"] = lookup_table_spectra_high_res.shape\n",
    "\n",
    "    fp = np.memmap(\n",
    "        output_folder + \"array_cumulated_lookup_mz_image_\" + str(slice_index) + \".mmap\",\n",
    "        dtype=\"float32\",\n",
    "        mode=\"w+\",\n",
    "        shape=cumulated_image_lookup_table_high_res.shape,\n",
    "    )\n",
    "    fp[:] = cumulated_image_lookup_table_high_res[:]\n",
    "    fp.flush()\n",
    "    dic_slices[slice_index][\n",
    "        \"array_cumulated_lookup_mz_image_shape\"\n",
    "    ] = cumulated_image_lookup_table_high_res.shape\n",
    "\n",
    "# Pickle the dict of lightweight data\n",
    "with open(output_folder + \"light_arrays.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(dic_slices, handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean temporary folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = False\n",
    "if clean:\n",
    "    delete_all_files_in_folder(input_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c1aa729cc35b9a783763c24c4069d7da678acf641f89d4e1df25bf02079ad65"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
